<!DOCTYPE html>

<html>
	<body>
		<h1>Coursera: Practical Machine Learning</h1>
		<h2>Final Project Writeup - Andrew Botelho</h2>
		
		<p>
			I started off by downloading the training (pml-training.csv) and 
			testing (pml-testing.csv) datasets into R.  The testing dataset is used for 
			predicting the 20 test cases later on, so we can put that aside for now.
		</p>
		<p>
			Next I took the training set and decided to split that into its own training 
			and testing datasets.  This way I could train my model on one part of the training set 
			and test the model on the other portion of the training set. 
			I used the createDataPartition() function to split the training dataset into 70/30 portions. 
			I named the new training and testing datasets <i>subTraining</i> and <i>subTesting</i>, respectively.
		</p>
		<p>
			With 160 different variables available in the dataset, the first task was to analyze these 
			variables and decide which ones I wanted to use. Using ggplot2's qplot() function, 
			I looked at many different scatter plots of the variables and how they affected 'classe' - the variable we are trying to predict. 
			I did not bother analyzing any variables that had 'NA' or blank rows in the dataset.
		</p>
		
		<p>
			While looking at the different scatterplots, I tried to look for plots that created different kinds of 
			clusters around the classe variable in the dataset - basically trying to notice trends that may be useful 
			for predicting the classe.  Here are some of the plots I noticed:
		</p>
		
		<figure>
			<img src="plot1.jpeg" alt="Plot 1">
			<figcaption>qplot(roll_belt, pitch_belt, colour=classe, data=subTraining)</figcaption>
		</figure>
		
		<figure>
			<img src="plot2.jpeg" alt="Plot 2">
			<figcaption>qplot(accel_belt_x, accel_belt_y, colour=classe, data=subTraining)</figcaption>
		</figure>
		
		<figure>
			<img src="plot3.jpeg" alt="Plot 3">
			<figcaption>qplot(magnet_belt_x, magnet_belt_y, colour=classe, data=subTraining)</figcaption>
		</figure>
		
		<figure>
			<img src="plot4.jpeg" alt="Plot 4">
			<figcaption>qplot(magnet_arm_x, magnet_arm_y, colour=classe, data=subTraining)</figcaption>
		</figure>
		
		<p>
			Regardless of the plots, I decided to use every variable that did not have blank or NA fields for my first model. 
			Since I noticed that some of the plots seemed to have clusters around the different classes, I thought 
			that a decision tree may be useful in making splits that can classify rows into the different clusters. 
			Therefore I decided to use a Random Forest for my first model. I trained this model against the 
			<i>subTraining</i> dataset (See R code).
		</p>
		
		<p>
			To test the effectiveness of this model, I ran a prediction using my Random Forest model against the <i>subTesting</i> dataset. 
			Here is the confusion matrix of the predicted vs. actual for the subTesting dataset:
		</p>
		
		<figure>
			<img src="confusion_matrix.JPG" alt="Confusion Matrix">
			<figcaption>Confusion Matrix for the predicted vs. actual. The matrix reveals a 98% accuracy.</figcaption>
		</figure>
		
		<p>
			As you can see in the figure above, my Random Forest model had 98% accuracy when predicting 
			the classe variable in the subTesting dataset.  Therefore, I estimate my <b>out-of-sample error rate</b> 
			to be 98%.
		</p>
		
		<p>
			I trained a few more Random Forest models using fewer variables from the subTraining dataset, but this always resulted in 
			a lower accuracy when predicting against the subTesting dataset.  Therefore, I decided to stick with the first Random Forest model I made 
			that uses all variables from the subTraining dataset that do not contain NA or blank values.  When running this Random Forest model against the 
			20 test cases in the testing dataset, I was 20/20.
		</p>
		
	</body>
</html>
